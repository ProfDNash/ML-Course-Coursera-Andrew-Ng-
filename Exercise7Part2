import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
import imageio


#### Machine Learning Exercise 7 Part 2: PCA and K-means Clustering ####

##Feature Normalization##
def featureNormalize(X):
    mu = np.mean(X,axis=0)
    std= np.std(X,axis=0)
    X_norm=(X-mu)/std
    return X_norm, mu, std

##Principal Component Analysis##
def pca(X):
    m,n = X.shape
    Sigma = np.matmul(X.transpose(), X)/m
    U, S, V = np.linalg.svd(Sigma, full_matrices=False)
    return U, S, V

##Project Data using top K eigenvectors from PCA##
def projectData(X, U, K):
    U_reduce = U[:,:K]
    Z = np.matmul(X,U_reduce)
    return Z

##Recover an approximation of the data using the eigenvectors from PCA##
def recoverData(Z, U, K):
    U_reduce = U[:,:K]
    X_rec = np.matmul(Z,U_reduce.transpose())
    return X_rec

##display 2D data in a nice grid##
def displayData(X):#,example_width):  
    fig, ax = plt.subplots()
    #plt.gray()
    m, n = np.shape(X)
    ## compute size of one example
    example_width = int(np.sqrt(n))  
    example_height = int(n/example_width)
    ## compute number of examples
    display_rows = int(np.floor(np.sqrt(m)))
    display_cols = int(np.ceil(m/display_rows))
    ##add padding between each image
    pad = 1
    ##setup blank display
    display_array = -np.ones((pad+display_rows*(example_height+pad), pad + display_cols*(example_width+pad)))
    ##copy each example into a patch in the display
    curr_ex=0
    for i in range(display_rows):
        for j in range(display_cols):
            if curr_ex>=m:
                break
            ##otherwise, copy the patch
            max_val=np.max(np.abs(X[curr_ex,:])) ##find max value in this example
            ex_start_ht=pad + i * (example_height + pad)
            ex_start_wd=pad + j * (example_width + pad)
            display_array[ex_start_ht:ex_start_ht + example_height, ex_start_wd:ex_start_wd+example_height] = X[curr_ex,:].reshape(example_height, example_width)/max_val
            curr_ex=curr_ex+1
        if curr_ex>=m:
            break
    plt.imshow(np.flipud(np.rot90(display_array)), cmap='gray') ##rotated/flipped to get orientation right

##Compute centroid memberships for every example based on given centroids##
def findClosestCentroids(X, centroids):
    K = centroids.shape[0]
    m = X.shape[0] ##number of examples
    idx = np.zeros(m)
    for i in range(m):
        dist = np.zeros(K)
        for k in range(K):
            dist[k] = np.matmul(X[i,:] - centroids[k,:], (X[i,:] - centroids[k,:]).transpose())
        idx[i] = np.argmin(dist)
    return idx

##Compute the new centroids based on given memberships##
def computeCentroids(X, idx, K):
    n = X.shape[1]
    centroids = np.zeros((K,n))
    for k in range(K):
        clusterk = np.where(idx==k)[0] ##find all examples in cluster k
        centroids[k,:] = np.sum(X[clusterk,:],0)/len(clusterk)
    return centroids

##Plot K-Means Clustering Progress##
def plotProgress(X, centroids, previous, idx, K, i):
    plt.scatter(X[:,0], X[:,1], c=idx)  ##plot examples colored by membership
    plt.scatter(centroids[:,0], centroids[:,1], c='r', marker='x', s=100) ##plot centroids
    ##plot lines from old centroids to new ones##
    for j in range(centroids.shape[0]):
        plt.plot([previous[j,0], centroids[j,0]], [previous[j,1], centroids[j,1]], c='r')
    plt.title('Iteration Number {}'.format(i+1))
    plt.show()
    

##K-Means clustering algorithm##
def runKMeans(X, centroids, max_iters, plot_progress=False):
    m,n = X.shape
    K = centroids.shape[0]
    previous_centroids=centroids
    for i in range(max_iters):
        print('K-means iteration {} of {}'.format(i+1, max_iters))
        idx = findClosestCentroids(X, centroids)
        ##Optionally plot progress##
        if plot_progress:
            plotProgress(X, centroids, previous_centroids, idx, K, i)
            previous_centroids = centroids
            input('Press enter to continue.')
        centroids = computeCentroids(X, idx, K)
    return centroids, idx


##Load and Visualize the Data##
file = loadmat('ex7data1.mat')
X = file['X']
plt.plot(X[:,0], X[:,1], 'o')
plt.show()

##Normalize Features and Run PCA##
input('\nProgram paused.  Press enter to continue.\n')
print('Running PCA on example dataset.')
##First normalize X##
X_norm, mu, sigma = featureNormalize(X)
##Run PCA##
U, S, V = pca(X_norm)
##Draw eigenvectors on top of the data##
plt.plot(X[:,0], X[:,1], 'o')
plt.plot([mu[0], mu[0]+1.5*S[0]*U[0,0]], [mu[1], mu[1]+1.5*S[0]*U[0,1]], '-k')
plt.plot([mu[0], mu[0]+1.5*S[1]*U[1,0]], [mu[1], mu[1]+1.5*S[1]*U[1,1]], '-k')
plt.show()
print('Top eigenvector U[:,0] = ', U[:,0])
print('(you should expect to see [-0.707107, -0.707107])')


##Dimension Reduction##
input('\nProgram paused.  Press enter to continue.\n')
print('Dimension reduction on example dataset...')
plt.plot(X_norm[:,0], X_norm[:,1], 'o')
plt.show()

##Project the Data onto K-dimensions##
K = 1
Z = projectData(X_norm, U, K)
print('Projection of the first example: ', Z[0])
print('(this value should be about 1.481274)')

X_rec = recoverData(Z, U, K)
print('Approximation of the first example: ', X_rec[0,:])
print('(this value should be about [-1.047419, -1.047419])')

##Draw lines connecting the projected points to the original points##
plt.plot(X_norm[:,0], X_norm[:,1], 'o')
for i in range(X_norm.shape[0]):
    plt.plot([X_norm[i,0], X_rec[i,0]], [X_norm[i,1], X_rec[i,1]], '--k')
plt.show()
    

##Load and Visualize Face Data##
input('\nProgram paused.  Press enter to continue.\n')
print('Loading Face Dataset...')
file2 = loadmat('ex7faces.mat')
Xf = file2['X']
##Display the first 100 faces in the dataset
displayData(Xf[0:100,:])
plt.show()


##PCA on Face Data: Eigenface##
input('\nProgram paused.  Press enter to continue.\n')
print('Running PCA on Face Dataset...')
Xf_norm, mu, sigma = featureNormalize(Xf)
Uf, Sf, Vf = pca(Xf_norm)
##Visualize the top 36 eigenvectors##
displayData(Uf[:,:36].transpose())
plt.show()


##Dimension Reduction for Faces##
input('\nProgram paused.  Press enter to continue.\n')
print('Dimension reduction for face dataset...')
K=100
Zf = projectData(Xf, Uf, K)
print('The projected data Z has size: ', Z.shape)


##Visualization of Faces after PCA Dimension Reduction##
input('\nProgram paused.  Press enter to continue.\n')
print('Visualizing the projected (reduced dimension) faces.')
Xf_rec = recoverData(Zf, Uf, K)
displayData(Xf_norm[:100,:])
plt.title('Original Faces')
plt.show()


displayData(Xf_rec[:100,:])
plt.title('Recovered Faces')
plt.show()


##Use PCA for Visualization##
input('\nProgram paused.  Press enter to continue.\n')
A = imageio.imread('bird_small.png')
A = A/255  ##convert all numbers to 0-1##
X = A.reshape((A.shape[0]*A.shape[1], 3))
K = 16
max_iters=10
##choose K random examples to be the initial centroids##
randK = np.random.permutation(np.arange(X.shape[0]))[:K] 
init_centroids = X[randK,:]
centroids, idx = runKMeans(X, init_centroids, max_iters)
##Sample 1000 random indexes (working with all the data is too expensive)##
sel = np.random.permutation(np.arange(X.shape[0]))[:1000]
##Visualize centroid memberships in 3D for the sample of examples##
ax = plt.axes(projection='3d')
ax.scatter3D(X[sel,0], X[sel,1], X[sel,2], c=idx[sel])
plt.title('Pixel dataset plotted in 3D.  Color shows centroid memberships.')
plt.show()


##Run PCA to project Pixel Dataset to 2D##
input('\nProgram paused.  Press enter to continue.\n')
print('Reducing dataset to 2D using PCA...')
X_norm, mu, sigma = featureNormalize(X)
U, S, V = pca(X_norm)
Z = projectData(X_norm, U, 2)
plt.scatter(Z[sel,0], Z[sel,1], c=idx[sel])
plt.title('Pixel dataset plotted in 2D using PCA')
plt.show()
